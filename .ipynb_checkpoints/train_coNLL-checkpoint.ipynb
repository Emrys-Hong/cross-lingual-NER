{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/pytorch4/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/root/anaconda3/envs/pytorch4/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/root/anaconda3/envs/pytorch4/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "/root/anaconda3/envs/pytorch4/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/root/anaconda3/envs/pytorch4/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from fastai.text import *\n",
    "from sebastian.eval import eval_ner\n",
    "from model.data_utils import minibatches, pad_sequences, get_chunks\n",
    "from model.config import Config\n",
    "from model.data_utils import CoNLLDataset\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler, BatchSampler\n",
    "import torch.nn.utils.rnn as rnn_utils  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = CoNLLDataset(config.filename_train, config.processing_word,\n",
    "                         config.processing_tag, config.max_iter)\n",
    "val = CoNLLDataset(config.filename_dev, config.processing_word,\n",
    "                         config.processing_tag, config.max_iter)\n",
    "test = CoNLLDataset(config.filename_test, config.processing_word,\n",
    "                         config.processing_tag, config.max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Minibatch(object):\n",
    "    def __init__(self, data, minibatch_size):\n",
    "        self.data = data\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.length = None\n",
    "        \n",
    "    def __iter__(self):\n",
    "        x_batch, y_batch = [], []\n",
    "        for (x, y) in self.data:\n",
    "            if len(x_batch) == self.minibatch_size:\n",
    "                char_ids, word_ids = zip(*x_batch)\n",
    "                word_ids, sequence_lengths = pad_sequences(word_ids, 0)\n",
    "                char_ids, word_lengths = pad_sequences(char_ids, pad_tok=0,\n",
    "                    nlevels=2)\n",
    "                lbl_ids, lbl_lengths = pad_sequences(y_batch, 0)\n",
    "                word_ids = np.concatenate([np.array(word_ids)[:,:,np.newaxis], np.array(char_ids)], axis=-1)\n",
    "                print(T(sequence_lengths).size())\n",
    "                print(T(word_lengths).size())\n",
    "                print(T(lbl_lengths).size())\n",
    "                print(T(word_ids).size())\n",
    "                yield T(word_ids), T(lbl_ids).view(-1)\n",
    "                x_batch, y_batch = [], []\n",
    "\n",
    "            if type(x[0]) == tuple:\n",
    "                x = zip(*x)\n",
    "            x_batch += [x]\n",
    "            y_batch += [y]\n",
    "\n",
    "        if len(x_batch) != 0:\n",
    "            char_ids, word_ids = zip(*x_batch)\n",
    "            word_ids, sequence_lengths = pad_sequences(word_ids, 0)\n",
    "            char_ids, word_lengths = pad_sequences(char_ids, pad_tok=0,\n",
    "                nlevels=2)\n",
    "            lbl_ids, lbl_lengths = pad_sequences(y_batch, 0)\n",
    "            word_ids = np.concatenate([np.array(word_ids)[:,:,np.newaxis], np.array(char_ids)], axis=-1)\n",
    "            yield T(word_ids), T(lbl_ids).view(-1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.length == None:\n",
    "            self.length = 0\n",
    "            for _ in self:\n",
    "                self.length += 1\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '/home/'\n",
    "trn_dl = Minibatch(train, 20)\n",
    "val_dl = Minibatch(val, 20)\n",
    "test_dl = Minibatch(test, 20)\n",
    "md = ModelData(dir_path, trn_dl, val_dl, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20])\n",
      "torch.Size([20, 61])\n",
      "torch.Size([20])\n",
      "torch.Size([20, 61, 19])\n"
     ]
    }
   ],
   "source": [
    "for i in trn_dl:\n",
    "    pass\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NER_model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(NER_model, self).__init__()\n",
    "        self.config = config\n",
    "        self.idx_to_tag = {idx: tag for tag, idx in\n",
    "                           self.config.vocab_tags.items()}\n",
    "        self.get_word_embeddings()\n",
    "        self.get_logits()\n",
    "        \n",
    "    def get_word_embeddings(self):\n",
    "        # get word embeding\n",
    "        _word_embedding = V(self.config.embeddings, requires_grad=True)\n",
    "        self.word_embedding = nn.Embedding.from_pretrained(_word_embedding)\n",
    "        \n",
    "        # get char embedding\n",
    "        self._char_embedding = nn.Embedding(self.config.nchars, self.config.dim_char)\n",
    "        self.char_embedding = nn.LSTM(input_size=self.config.dim_char, hidden_size=self.config.hidden_size_char,\n",
    "                                     num_layers=1, batch_first=True, # not sure here whether batch is first\n",
    "                                     bidirectional=True)\n",
    "        \n",
    "    def get_logits(self):\n",
    "        self.rnn = nn.LSTM(input_size=self.config.dim_word+self.config.dim_char*2,\n",
    "                          hidden_size=self.config.hidden_size_lstm,\n",
    "                          num_layers=1, batch_first=True, # not sure whether batch is first\n",
    "                          bidirectional=True)\n",
    "        self.dropout = nn.Dropout(self.config.dropout if self.train else 0)\n",
    "        self.linear = nn.Linear(self.config.hidden_size_lstm*2, self.config.ntags)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        char = input[:, :, 1:]\n",
    "        words = input[:, :, 0]\n",
    "        bs, sl, _ = char.size()\n",
    "        char = char.view(-1, _)\n",
    "        _char_embedding = self._char_embedding(char)\n",
    "        \n",
    "        char_embedding, (h_n, cell_n) = self.char_embedding(_char_embedding)\n",
    "        char_embedding = h_n.view(bs, sl, -1)\n",
    "        word_embedding = self.word_embedding(words)\n",
    "\n",
    "        # concat word embeddings and char embeddings\n",
    "        word_embedding = torch.cat([word_embedding, char_embedding], dim=-1)\n",
    "        word_embedding_dp = self.dropout(word_embedding)\n",
    "        \n",
    "        out, (n_h, n_cell) = self.rnn(word_embedding_dp)\n",
    "        out_dp = self.dropout(out)\n",
    "        out = self.linear(out_dp)\n",
    "        return out.view(out.size(0)*out.size(1), out.size(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model = NER_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.ntags = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### rewrite RNN Learner #####\n",
    "'''rewrite load_encoder to load the encoding modules'''\n",
    "class RNN_Learner_bidir(Learner):\n",
    "    def __init__(self, data, models, **kwargs):\n",
    "        super().__init__(data, models, **kwargs)\n",
    "\n",
    "    def _get_crit(self, data): return F.cross_entropy\n",
    "    def fit(self, *args, **kwargs): return super().fit(*args, **kwargs, seq_first=True)\n",
    "\n",
    "    def save_encoder(self, name_rnn, name_linear): \n",
    "        torch.save(self.model.rnn.state_dict(), name_rnn)\n",
    "        torch.save(self.model.linear.state_dict(), name_linear)\n",
    "        \n",
    "    def load_encoder(self, name_rnn, name_linear): \n",
    "        self.model.rnn.load_state_dict(torch.load(name_rnn))\n",
    "        self.model.linear.load_state_dict(torch.load(name_linear))\n",
    "##### end #####\n",
    "\n",
    "\n",
    "##### rewrite textmodel #####\n",
    "'''get layer groups'''\n",
    "class TextModel_bidir(BasicModel):\n",
    "    def get_layer_groups(self):\n",
    "        return [(self.model._char_embedding),(self.model.char_embedding), \n",
    "                (self.model.word_embedding),(self.model.rnn),(self.model.linear)]\n",
    "\n",
    "def freeze_all_but(learner, n):\n",
    "    c=learner.get_layer_groups()\n",
    "    for l in c: set_trainable(l, False)\n",
    "    set_trainable(c[n], True)\n",
    "    \n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "learn = RNN_Learner_bidir(md, TextModel_bidir(to_gpu(ner_model)), opt_fn=opt_fn)\n",
    "# learn.load_encoder('results/eng_rnn_params.pkl', 'results/eng_linear_params.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_all_but(learn, -1)\n",
    "# learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88aef6c79ed346c6bd2f9092d02e1396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                    \n",
      "    0      0.788397   0.743344   0.930329  \n",
      "    1      0.51935    0.450403   0.935868                    \n",
      "    2      0.475922   0.417977   0.937061                    \n",
      "    3      0.399008   0.331141   0.937078                    \n",
      "    4      0.369746   0.306766   0.937094                    \n",
      "    5      0.361044   0.300231   0.937102                    \n",
      "    6      0.35779    0.299964   0.937102                    \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2999637363622961, 0.9371023906371386]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(0.001, 3, metrics=[accuracy], cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('results/eng_rnn_params.pkl', 'results/eng_linear_params.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2tag = [o for i,o in ner_model.idx_to_tag.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1 measure overall: 0.9649502379921613\n",
      "{'precision-MISC': 0.0, 'recall-MISC': 0.0, 'f1-measure-MISC': 0.0, 'precision-PER': 0.9998980943646184, 'recall-PER': 0.9852394818756903, 'f1-measure-PER': 0.9925146672061002, 'precision-LOC': 0.0, 'recall-LOC': 0.0, 'f1-measure-LOC': 0.0, 'precision-ORG': 0.0, 'recall-ORG': 0.0, 'f1-measure-ORG': 0.0, 'precision-overall': 0.9998980943646184, 'recall-overall': 0.9323628347174975, 'f1-measure-overall': 0.9649502379921613}\n",
      "Test token-level accuracy of NER model: 0.9386.\n"
     ]
    }
   ],
   "source": [
    "eval_ner(learn, id2tag, is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "import torch.nn.utils.rnn as rnn_utils  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(in_size, hidden_size, bidirectional, num_layers)\n",
    "packed = rnn_utils.pack_padded_sequence(padded, lengths)\n",
    "packed_out, packed_hidden = lstm(packed)\n",
    "unpacked, unpacked_len = rnn_utils.pad_packed_sequence(packed_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
